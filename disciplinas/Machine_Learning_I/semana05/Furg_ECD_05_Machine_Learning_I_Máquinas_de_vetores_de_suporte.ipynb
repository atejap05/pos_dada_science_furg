{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "nav_menu": {},
    "toc": {
      "navigate_menu": true,
      "number_sections": true,
      "sideBar": true,
      "threshold": 6,
      "toc_cell": false,
      "toc_section_display": "block",
      "toc_window_display": false
    },
    "colab": {
      "name": "Furg - ECD 05 - Machine Learning I - Máquinas de vetores de suporte",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atejap05/pos_data_science_furg/blob/main/Furg_ECD_05_Machine_Learning_I_M%C3%A1quinas_de_vetores_de_suporte.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rZ8J0nL_rfGZ"
      },
      "source": [
        "# Curso de Especialização em Ciência de Dados - FURG\n",
        "## Machine Learning I - Máquinas de vetores de suporte\n",
        "### Prof. Marcelo Malheiros\n",
        "\n",
        "Código adaptado de Aurélien Geron (licença Apache-2.0)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "786PyRb_rfGd"
      },
      "source": [
        "# Inicialização\n",
        "\n",
        "Aqui importamos as bibliotecas fundamentais de Python para este _notebook_:\n",
        "\n",
        "- NumPy: suporte a vetores, matrizes e operações de Álgebra Linear\n",
        "- Matplotlib: biblioteca de visualização de dados\n",
        "- Pandas: pacote estatístico e de manipulação de DataFrames\n",
        "- Scikit-Learn: biblioteca com algoritmos de Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3oLeGJYsrfGf"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcbcCL8vrfGg"
      },
      "source": [
        "# Classificação usando SVM\n",
        "\n",
        "**Máquinas de vetores de suporte** (SVM) é uma técnica (ou algoritmo) clássico de Machine Learning.\n",
        "\n",
        "É bastante versátil e eficiente para _datasets_ de tamanho pequeno ou médio. Além disso é útil para tarefas de classificação (que veremos a seguir) como de regressão (ao final deste _notebook_)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jahoQELlrfGi"
      },
      "source": [
        "## Classificação linear\n",
        "\n",
        "Aqui vamos usar um caso especializado de classificador SVM linear, chamado `LinearSVC`.\n",
        "\n",
        "O objetivo é criar uma visualização da área de decisão em torno de algumas instâncias.\n",
        "\n",
        "O conjunto de dados aqui é interno à biblioteca `Scikit-Learn`: é o mesmo _dataset_ sobre características de três espécies de flores, chamado IRIS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PadKUagzrfGi"
      },
      "source": [
        "# carregamento e preparação dos dados\n",
        "from sklearn import datasets\n",
        "iris = datasets.load_iris()\n",
        "\n",
        "X_data = iris['data'][:, (2, 3)]  # comprimento da pétala, largura da pétala\n",
        "y_data = (iris['target'] == 2).astype(np.float64)  # Iris virginica"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaUer8NjrfGj"
      },
      "source": [
        "# carregamento e preparação dos dados\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "])\n",
        "\n",
        "X = pipeline.fit_transform(X_data)\n",
        "y = y_data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2ifihcxfrfGk"
      },
      "source": [
        "# criação do modelo e treinamento\n",
        "\n",
        "from sklearn.svm import LinearSVC\n",
        "\n",
        "svm_clf = LinearSVC(C=1, loss='hinge', random_state=42)\n",
        "svm_clf.fit(X, y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ty7muOVBrfGk"
      },
      "source": [
        "# predição\n",
        "svm_clf.predict([[5.5, 1.7]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XrnnoRkPrfGm"
      },
      "source": [
        "## Visualização da classificação linear\n",
        "\n",
        "Experimente trocar o valor do parâmetro `C` e executar novamente as células até gerar a figura. Valores maiores de `C` fazem com que a área de decisão fique mais estreita.\n",
        "\n",
        "Experimente com 0.1, 10, 100 e 1000, por exemplo. Note que para alguns valores a escolha da fronteira de decisão fica bem ruim."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S-dJEP5DrfGn"
      },
      "source": [
        "svm_clf = LinearSVC(C=1, loss=\"hinge\", random_state=42)\n",
        "\n",
        "# note que aqui usamos um atalho: o modelo é colocado como a última parte do pipeline\n",
        "scaler = StandardScaler()\n",
        "scaled_svm_clf = Pipeline([\n",
        "    ('scaler', scaler),\n",
        "    ('linear_svc', svm_clf),\n",
        "])\n",
        "\n",
        "# a chamada a seguir primeiro transforma os dados (brutos) e então treina o modelo\n",
        "scaled_svm_clf.fit(X_data, y_data);\n",
        "\n",
        "# NOTA: é esperada a impressão de advertências abaixo para valores grandes de C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rBcI1xnKrfGo"
      },
      "source": [
        "# conversão para parâmetros sem escalonamento\n",
        "b = svm_clf.decision_function([-scaler.mean_ / scaler.scale_])\n",
        "w = svm_clf.coef_[0] / scaler.scale_\n",
        "svm_clf.intercept_ = np.array([b])\n",
        "svm_clf.coef_ = np.array([w])\n",
        "\n",
        "# busca dos vetores de suporte (LinearSVC não faz isso automaticamente)\n",
        "t = y * 2 - 1\n",
        "support_vectors_idx = (t * (X_data.dot(w) + b) < 1).ravel()\n",
        "svm_clf.support_vectors_ = X_data[support_vectors_idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cihxIVFcrfGp"
      },
      "source": [
        "# função auxiliar\n",
        "\n",
        "def plot_svc_decision_boundary(svm_clf, xmin, xmax):\n",
        "    w = svm_clf.coef_[0]\n",
        "    b = svm_clf.intercept_[0]\n",
        "\n",
        "    x0 = np.linspace(xmin, xmax, 200)\n",
        "    decision_boundary = -w[0]/w[1] * x0 - b/w[1]\n",
        "\n",
        "    margin = 1/w[1]\n",
        "    gutter_up = decision_boundary + margin\n",
        "    gutter_down = decision_boundary - margin\n",
        "\n",
        "    svs = svm_clf.support_vectors_\n",
        "    plt.scatter(svs[:, 0], svs[:, 1], s=180, facecolors='#FFAAAA')\n",
        "    plt.plot(x0, decision_boundary, \"k-\", linewidth=2)\n",
        "    plt.plot(x0, gutter_up, \"k--\", linewidth=2)\n",
        "    plt.plot(x0, gutter_down, \"k--\", linewidth=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_qXRKLbfrfGp"
      },
      "source": [
        "# plotagem\n",
        "\n",
        "fig, axes = plt.subplots(figsize=(10,4))\n",
        "plt.plot(X_data[:, 0][y_data==1], X_data[:, 1][y_data==1], \"g^\", label=\"$Iris\\ virginica$\")\n",
        "plt.plot(X_data[:, 0][y_data==0], X_data[:, 1][y_data==0], \"bs\", label=\"$Iris\\ versicolor$\")\n",
        "plot_svc_decision_boundary(svm_clf, 4, 5.9)\n",
        "plt.xlabel(\"Comprimento da pétala\", fontsize=14)\n",
        "plt.ylabel(\"Largura da pétala\", fontsize=14)\n",
        "plt.legend(loc=\"upper left\", fontsize=14)\n",
        "plt.title(\"$C = {}$\".format(svm_clf.C), fontsize=16)\n",
        "plt.axis([4, 5.9, 0.8, 2.8])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJvQ9cpcrfGp"
      },
      "source": [
        "## Classificação não linear - usando adição de features polinomiais\n",
        "\n",
        "Aqui vamos usar um caso geral de classificador SVM, chamado `SVC`.\n",
        "\n",
        "O objetivo é criar uma visualização da área de decisão em torno de algumas instâncias para um _dataset_ complexo.\n",
        "\n",
        "O conjunto de dados aqui é sintético, chamado `moons` e gerado pela biblioteca `Scikit-Learn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bx0qribirfGq"
      },
      "source": [
        "# carregamento e visualização dos dados\n",
        "\n",
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=100, noise=0.15, random_state=42)\n",
        "\n",
        "def plot_dataset(X, y, axes):\n",
        "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"bs\")\n",
        "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"g^\")\n",
        "    plt.axis(axes)\n",
        "    plt.grid(True, which='both')\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=20)\n",
        "    plt.ylabel(r\"$x_2$\", fontsize=20, rotation=0)\n",
        "\n",
        "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2PKmbExrfGr"
      },
      "source": [
        "# processamento (com adição de novas features de grau 3), criação do modelo e treinamento\n",
        "\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "# este pipeline já inclui o modelo no final\n",
        "polynomial_svm_clf = Pipeline([\n",
        "    (\"poly_features\", PolynomialFeatures(degree=3)),\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm_clf\", LinearSVC(C=1, loss=\"hinge\", random_state=42))\n",
        "])\n",
        "\n",
        "# transforma os dados (brutos) e treina o modelo\n",
        "polynomial_svm_clf.fit(X, y);\n",
        "\n",
        "# NOTA: é esperada a impressão de advertências abaixo para valores grandes de C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkUc2FnbrfGr"
      },
      "source": [
        "# plotagem\n",
        "\n",
        "def plot_predictions(clf, axes):\n",
        "    x0s = np.linspace(axes[0], axes[1], 100)\n",
        "    x1s = np.linspace(axes[2], axes[3], 100)\n",
        "    x0, x1 = np.meshgrid(x0s, x1s)\n",
        "    X = np.c_[x0.ravel(), x1.ravel()]\n",
        "    y_pred = clf.predict(X).reshape(x0.shape)\n",
        "    y_decision = clf.decision_function(X).reshape(x0.shape)\n",
        "    plt.contourf(x0, x1, y_pred, cmap=plt.cm.brg, alpha=0.2)\n",
        "    plt.contourf(x0, x1, y_decision, cmap=plt.cm.brg, alpha=0.1)\n",
        "\n",
        "plot_predictions(polynomial_svm_clf, [-1.5, 2.5, -1, 1.5])\n",
        "plot_dataset(X, y, [-1.5, 2.5, -1, 1.5])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bSPqiE3_rfGs"
      },
      "source": [
        "## Classificação não linear - usando kernel polinomial\n",
        "\n",
        "Aqui é mostrado o mesmo classificador `SVC`, agora usando um esquema mais eficiente de cálculo usando um _kernel_ polinomial.\n",
        "\n",
        "Dois modelos são treinados e mostrados lado a lado, com hiperparâmetros diferentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXd0Rzx5rfGs"
      },
      "source": [
        "# estes pipelines já incluem o modelo no final\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "poly_kernel_svm_clf1 = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm_clf1\", SVC(kernel=\"poly\", degree=3, coef0=1, C=5))\n",
        "])\n",
        "poly_kernel_svm_clf2 = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"svm_clf2\", SVC(kernel=\"poly\", degree=10, coef0=100, C=5))\n",
        "])\n",
        "\n",
        "# transforma os dados (brutos) e treina cada modelo\n",
        "poly_kernel_svm_clf1.fit(X, y)\n",
        "poly_kernel_svm_clf2.fit(X, y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lyIUmNkYrfGt"
      },
      "source": [
        "# plotagem das duas figuras\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(10.5, 4), sharey=True)\n",
        "plt.sca(axes[0])\n",
        "plot_predictions(poly_kernel_svm_clf1, [-1.5, 2.45, -1, 1.5])\n",
        "plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\n",
        "plt.title(\"modelo 1\", fontsize=18)\n",
        "plt.sca(axes[1])\n",
        "plot_predictions(poly_kernel_svm_clf2, [-1.5, 2.45, -1, 1.5])\n",
        "plot_dataset(X, y, [-1.5, 2.4, -1, 1.5])\n",
        "plt.title(\"modelo 2\", fontsize=18)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1UnxtjjrfGt"
      },
      "source": [
        "## Classificação não linear - usando kernel gaussiano\n",
        "\n",
        "Aqui é mostrado o mesmo classificador `SVC`, agora usando o esquema usando um _kernel_ gaussiano.\n",
        "\n",
        "Quatro modelos são treinados com combinações de hiperparâmetros diferentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uex5d2FtrfGt"
      },
      "source": [
        "# hiperparâmetros para experimentar\n",
        "gamma1, gamma2 = 0.1, 5\n",
        "C1, C2 = 0.001, 1000\n",
        "hyperparams = (gamma1, C1), (gamma1, C2), (gamma2, C1), (gamma2, C2)\n",
        "\n",
        "# estes pipelines já incluem o modelo no final\n",
        "svm_clfs = []\n",
        "for gamma, C in hyperparams:\n",
        "    rbf_kernel_svm_clf = Pipeline([\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"svm_clf\", SVC(kernel=\"rbf\", gamma=gamma, C=C))\n",
        "    ])\n",
        "    \n",
        "    # transforma os dados (brutos) e treina cada modelo\n",
        "    rbf_kernel_svm_clf.fit(X, y)\n",
        "    \n",
        "    # guarda modelo treinado para posterior visualização\n",
        "    svm_clfs.append(rbf_kernel_svm_clf)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "PVdhMcFcrfGt"
      },
      "source": [
        "# plotagem das quatro figuras\n",
        "\n",
        "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(10.5, 7), sharex=True, sharey=True)\n",
        "for i, svm_clf in enumerate(svm_clfs):\n",
        "    plt.sca(axes[i // 2, i % 2])\n",
        "    plot_predictions(svm_clf, [-1.5, 2.45, -1, 1.5])\n",
        "    plot_dataset(X, y, [-1.5, 2.45, -1, 1.5])\n",
        "    gamma, C = hyperparams[i]\n",
        "    plt.title(r\"$\\gamma = {}, C = {}$\".format(gamma, C), fontsize=16)\n",
        "    if i in (0, 1):\n",
        "        plt.xlabel(\"\")\n",
        "    if i in (1, 3):\n",
        "        plt.ylabel(\"\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URFKK3bwrfGu"
      },
      "source": [
        "# Regressão usando SVM\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCR73cSdrfGv"
      },
      "source": [
        "## Regressão linear\n",
        "\n",
        "Aqui um conjunto simples de valores aleatórios ao longo de uma linha é ajustado com o uso do algoritmo mais especializado `LinearSVR`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDlvPO65rfGv"
      },
      "source": [
        "# conjunto de dados\n",
        "\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(50, 1)\n",
        "y = (4 + 3 * X + np.random.randn(50, 1)).ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yd5kr4p2rfGw"
      },
      "source": [
        "# treinamento\n",
        "\n",
        "from sklearn.svm import LinearSVR\n",
        "\n",
        "svm_reg = LinearSVR(epsilon=1.5, random_state=42)\n",
        "svm_reg.fit(X, y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyB-UaePrfGw"
      },
      "source": [
        "## Visualização da regressão linear\n",
        "\n",
        "Aqui dois modelos são treinados, com diferença no hiperparâmetro `epsilon`. O resto do código é apenas para fazer a visualização dos dados junto com a área de ajuste (equivalente à área de decisão de um classificador)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRW9rrLprfGw"
      },
      "source": [
        "# treinamento dos modelos\n",
        "svm_reg1 = LinearSVR(epsilon=1.5, random_state=42)\n",
        "svm_reg2 = LinearSVR(epsilon=0.5, random_state=42)\n",
        "svm_reg1.fit(X, y)\n",
        "svm_reg2.fit(X, y)\n",
        "\n",
        "# informação sobre os vetores de suporte\n",
        "def find_support_vectors(svm_reg, X, y):\n",
        "    y_pred = svm_reg.predict(X)\n",
        "    off_margin = (np.abs(y - y_pred) >= svm_reg.epsilon)\n",
        "    return np.argwhere(off_margin)\n",
        "\n",
        "svm_reg1.support_ = find_support_vectors(svm_reg1, X, y)\n",
        "svm_reg2.support_ = find_support_vectors(svm_reg2, X, y)\n",
        "\n",
        "eps_x1 = 1\n",
        "eps_y_pred = svm_reg1.predict([[eps_x1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "EGvLCcy0rfGx"
      },
      "source": [
        "# plotagem\n",
        "\n",
        "def plot_svm_regression(svm_reg, X, y, axes):\n",
        "    x1s = np.linspace(axes[0], axes[1], 100).reshape(100, 1)\n",
        "    y_pred = svm_reg.predict(x1s)\n",
        "    plt.plot(x1s, y_pred, \"k-\", linewidth=2, label=r\"$\\hat{y}$\")\n",
        "    plt.plot(x1s, y_pred + svm_reg.epsilon, \"k--\")\n",
        "    plt.plot(x1s, y_pred - svm_reg.epsilon, \"k--\")\n",
        "    plt.scatter(X[svm_reg.support_], y[svm_reg.support_], s=180, facecolors='#FFAAAA')\n",
        "    plt.plot(X, y, \"bo\")\n",
        "    plt.xlabel(r\"$x_1$\", fontsize=18)\n",
        "    plt.legend(loc=\"upper left\", fontsize=18)\n",
        "    plt.axis(axes)\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
        "plt.sca(axes[0])\n",
        "plot_svm_regression(svm_reg1, X, y, [0, 2, 3, 11])\n",
        "plt.title(r\"$\\epsilon = {}$\".format(svm_reg1.epsilon), fontsize=18)\n",
        "plt.ylabel(r\"$y$\", fontsize=18, rotation=0)\n",
        "plt.annotate('', xy=(eps_x1, eps_y_pred), xycoords='data', xytext=(eps_x1, eps_y_pred - svm_reg1.epsilon),\n",
        "    textcoords='data', arrowprops={'arrowstyle': '<->', 'linewidth': 1.5})\n",
        "plt.text(0.91, 5.6, r\"$\\epsilon$\", fontsize=20)\n",
        "plt.sca(axes[1])\n",
        "plot_svm_regression(svm_reg2, X, y, [0, 2, 3, 11])\n",
        "plt.title(r\"$\\epsilon = {}$\".format(svm_reg2.epsilon), fontsize=18)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ece_BniOrfGx"
      },
      "source": [
        "## Regressão não linear\n",
        "\n",
        "Aqui um conjunto simples de valores aleatórios ao longo de uma curva quadrática é ajustado com o uso do algoritmo mais geral `SVR`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W11LlTbcrfGy"
      },
      "source": [
        "# conjunto de dados\n",
        "\n",
        "np.random.seed(42)\n",
        "X = 2 * np.random.rand(100, 1) - 1\n",
        "y = (0.2 + 0.1 * X + 0.5 * X**2 + np.random.randn(100, 1)/10).ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZo20_G2rfGy"
      },
      "source": [
        "# treinamento\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "svm_poly_reg = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
        "svm_poly_reg.fit(X, y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSPSpEferfGy"
      },
      "source": [
        "## Visualização da regressão não linear\n",
        "\n",
        "Aqui dois modelos são treinados, com diferença no hiperparâmetro `C`. O resto do código é apenas para fazer a visualização dos dados junto com a área de ajuste (equivalente à área de decisão de um classificador)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z62QKCuurfGz"
      },
      "source": [
        "# treinamento dos modelos\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "\n",
        "svm_poly_reg1 = SVR(kernel=\"poly\", degree=2, C=100, epsilon=0.1)\n",
        "svm_poly_reg2 = SVR(kernel=\"poly\", degree=2, C=0.01, epsilon=0.1)\n",
        "svm_poly_reg1.fit(X, y)\n",
        "svm_poly_reg2.fit(X, y);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBLv5SQdrfGz"
      },
      "source": [
        "# plotagem\n",
        "\n",
        "fig, axes = plt.subplots(ncols=2, figsize=(10, 4), sharey=True)\n",
        "plt.sca(axes[0])\n",
        "plot_svm_regression(svm_poly_reg1, X, y, [-1, 1, 0, 1])\n",
        "plt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(svm_poly_reg1.degree, svm_poly_reg1.C, svm_poly_reg1.epsilon),\n",
        "          fontsize=18)\n",
        "plt.ylabel(r\"$y$\", fontsize=18, rotation=0)\n",
        "plt.sca(axes[1])\n",
        "plot_svm_regression(svm_poly_reg2, X, y, [-1, 1, 0, 1])\n",
        "plt.title(r\"$degree={}, C={}, \\epsilon = {}$\".format(svm_poly_reg2.degree, svm_poly_reg2.C, svm_poly_reg2.epsilon),\n",
        "          fontsize=18)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}