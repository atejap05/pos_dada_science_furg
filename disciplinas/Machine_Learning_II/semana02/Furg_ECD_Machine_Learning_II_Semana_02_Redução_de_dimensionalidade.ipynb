{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Furg - ECD - Machine Learning II - Semana 02 - Redução de dimensionalidade",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atejap05/pos_data_science_furg/blob/main/disciplinas/Machine_Learning_II/semana02/Furg_ECD_Machine_Learning_II_Semana_02_Redu%C3%A7%C3%A3o_de_dimensionalidade.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Agk1mK57mHvQ"
      },
      "source": [
        "# Curso de Especialização em Ciência de Dados - FURG\n",
        "## Machine Learning I - Redução de dimensionalidade\n",
        "### Prof. Marcelo Malheiros\n",
        "\n",
        "Parte do código adaptada de Aurélien Geron (licença Apache-2.0)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0FKhP7jmHvW"
      },
      "source": [
        "# Inicialização\n",
        "\n",
        "Aqui importamos as bibliotecas fundamentais de Python para este _notebook_:\n",
        "\n",
        "- NumPy: suporte a vetores, matrizes e operações de Álgebra Linear\n",
        "- Matplotlib: biblioteca de visualização de dados\n",
        "- Pandas: pacote estatístico e de manipulação de DataFrames\n",
        "- Scikit-Learn: biblioteca com algoritmos de Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_-qDvlBtmHvX"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XXLDVgEumHvZ"
      },
      "source": [
        "# Maldição da dimensionalidade\n",
        "\n",
        "Aqui vamos fazer um experimento para medir quanto mais \"espaço\" adicionamos a um conjunto de dados quando aumentamos suas dimensões.\n",
        "\n",
        "Por simplicidade, vamos sortear um certo número de $m$ pontos aleatórios com coordenadas no intervalo $[0, 1)$ para cada uma das $n$ dimensões. Então, vamos medir a distância média entre todos os pares desses pontos.\n",
        "\n",
        "Isso equivale a criar um _dataset_ com $m$ instâncias (linhas) e $n$ atributos (colunas), ou seja, uma matriz de tamanho $m \\times n$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCd51LjQmHva"
      },
      "source": [
        "# função auxiliar\n",
        "def distância_média(pontos):\n",
        "    m = pontos.shape[0]\n",
        "    n = pontos.shape[1]\n",
        "    \n",
        "    medidas = 0\n",
        "    soma = 0.0\n",
        "    for i in range(0, m):\n",
        "        for j in range(0, m):\n",
        "            if i != j:\n",
        "                # a norma de ordem 2 corresponde à distância euclidiana\n",
        "                soma += np.linalg.norm(pontos[i] - pontos[j], ord=2)\n",
        "                medidas += 1\n",
        "    print(f'A distância média entre {m} pontos de dimensão {n} é {soma / medidas:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXoGptHkmHvb"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "# uma dimensão\n",
        "pontos = np.random.uniform(size=(100, 1))\n",
        "distância_média(pontos)\n",
        "\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.plot(pontos, np.zeros_like(pontos), 'o')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlvdixUVmHve"
      },
      "source": [
        "# duas dimensões\n",
        "pontos = np.random.uniform(size=(100, 2))\n",
        "distância_média(pontos)\n",
        "\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.plot(pontos[:,0], pontos[:,1], 'o')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9iniy8bmHvf"
      },
      "source": [
        "# três dimensões\n",
        "pontos = np.random.uniform(size=(100, 3))\n",
        "distância_média(pontos)\n",
        "\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "#ax.plot(pontos[:,0], pontos[:,1], pontos[:,2], 'o')\n",
        "ax.scatter(pontos[:,0], pontos[:,1], pontos[:,2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPvPQQxsmHvg"
      },
      "source": [
        "# dez dimensões\n",
        "pontos = np.random.uniform(size=(100, 10))\n",
        "distância_média(pontos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8hI-h3A-mHvi"
      },
      "source": [
        "# cem dimensões\n",
        "pontos = np.random.uniform(size=(100, 100))\n",
        "distância_média(pontos)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6sU0RrrymHvj"
      },
      "source": [
        "# Abordagem usando projeção\n",
        "\n",
        "Vamos exemplificar em seguida o uso de dois algoritmos que usam a abordagem de projeção: PCA e Kernel PCA.\n",
        "\n",
        "Primeiro, para demonstrar o PCA, vamos gerar um conjunto de dados sintéticos de **três dimensões**, armazenado na matriz `X`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPIhB5UJmHvj"
      },
      "source": [
        "np.random.seed(42)\n",
        "\n",
        "m = 60\n",
        "w1, w2 = 0.1, 0.3\n",
        "noise = 0.1\n",
        "\n",
        "angles = np.random.rand(m) * 3 * np.pi / 2 - 0.5\n",
        "X = np.empty((m, 3))\n",
        "X[:, 0] = np.cos(angles) + np.sin(angles)/2 + noise * np.random.randn(m) / 2\n",
        "X[:, 1] = np.sin(angles) * 0.7 + noise * np.random.randn(m) / 2\n",
        "X[:, 2] = X[:, 0] * w1 + X[:, 1] * w2 + noise * np.random.randn(m)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L7mN2sHmHvk"
      },
      "source": [
        "fig = plt.figure(figsize=(7, 7))\n",
        "ax = fig.add_subplot(projection='3d')\n",
        "ax.scatter(X[:,0], X[:,1], X[:,2], c='red')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GTpzS8mdmHvl"
      },
      "source": [
        "## Algoritmo PCA\n",
        "\n",
        "A biblioteca Scikit-Learn tem uma implementação completamente automatizada do PCA, que também é muito simples de usar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qyg8Tx4TmHvl"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "X2D = pca.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xnDUscr9mHvl"
      },
      "source": [
        "X2D[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWU40lmQmHvm"
      },
      "source": [
        "O objeto `PCA` dá acesso aos componentes principais (ou eixos) que foram computados para os dados:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fF4T_l_cmHvn"
      },
      "source": [
        "pca.components_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72c9XewMmHvn"
      },
      "source": [
        "Uma informação muito útil é a **proporção da variância explicada** de cada componente principal, disponível por meio da variável `explain_variance_ratio_`. Esta indica a proporção da variância do conjunto de dados ao longo de cada eixo associado a um componente principal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DX5eCV2SmHvn"
      },
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qW7FSY2GmHvo"
      },
      "source": [
        "Então 85,4% da variância do conjunto de dados está ao longo do primeiro eixo e 13,6% ao longo do segundo eixo. \n",
        "\n",
        "Isso deixa menos de 1% para o terceiro eixo, por isso é razoável supor que provavelmente carregue pouca informação. Então esta redução de dimensionalidade foi bem sucedida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t5yuD4WLmHvo"
      },
      "source": [
        "1 - pca.explained_variance_ratio_.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTUf0j_NmHvp"
      },
      "source": [
        "# plotagem dos dados reduzidos para duas dimensões\n",
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.plot(X2D[:,0], X2D[:,1], 'or')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g_UA7H1NmHvq"
      },
      "source": [
        "Agora vamos experimentar reduzir para um único componente, ou seja, restará uma única dimensão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wofYfLAqmHvq"
      },
      "source": [
        "pca = PCA(n_components=1)\n",
        "X1D = pca.fit_transform(X)\n",
        "X1D[:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5h6DViNmHvq"
      },
      "source": [
        "Como os dados são os mesmos, o primeiro (e único) componente também é o mesmo de antes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VR8KcIDmHvr"
      },
      "source": [
        "pca.components_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJg69sc5mHvs"
      },
      "source": [
        "pca.explained_variance_ratio_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lHq6At6kmHvt"
      },
      "source": [
        "Então 85,4% da variância do conjunto de dados foi mantida pelo primeiro eixo, e o restante, cerca de 14,5%, foi perdida:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P25VSFRZmHvt"
      },
      "source": [
        "1 - pca.explained_variance_ratio_.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ImlKy0JKmHvt"
      },
      "source": [
        "fig = plt.figure(figsize=(5, 5))\n",
        "plt.plot(X1D, np.zeros_like(X1D), 'or')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JW2Tm4GrmHvu"
      },
      "source": [
        "## Conjunto de dados MNIST\n",
        "\n",
        "Vamos aqui carregar a versão já disponível no Colaboratory do _dataset_ MNIST, que corresponde a imagens de dígitos manuscritos, já separados em conjuntos de treino e de teste.\n",
        "\n",
        "Este é um _dataset_ de alta dimensionalidade, uma vez que cada instância contém 28 * 28 = 784 atributos, além do rótulo indicando qual dígito de 0 a 9 este representa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8WlA_P2DmHvu"
      },
      "source": [
        "# cada linha representa um dígito: o rótulo está na coluna 0\n",
        "mnist_train = pd.read_csv('sample_data/mnist_train_small.csv', header=None)\n",
        "mnist_train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E945tfzcmHvv"
      },
      "source": [
        "mnist_train.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1uwUXCkhmHvv"
      },
      "source": [
        "# cada linha representa um dígito: o rótulo está na coluna 0\n",
        "mnist_test = pd.read_csv('sample_data/mnist_test.csv', header=None)\n",
        "mnist_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_zWDxXsmHvw"
      },
      "source": [
        "mnist_test.head(3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPFbrathmHvw"
      },
      "source": [
        "X_treino = mnist_train.iloc[:,1:].values\n",
        "y_treino = mnist_train.iloc[:,0].values\n",
        "X_teste = mnist_test.iloc[:,1:].values\n",
        "y_teste = mnist_test.iloc[:,0].values"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DCV1ghbmHvw"
      },
      "source": [
        "## Escolhendo o número certo de dimensões\n",
        "\n",
        "Em vez de escolher arbitrariamente o número de dimensões para reduzir, é geralmente preferível escolher o número de dimensões que somam uma porção suficientemente grande da variação (por exemplo, 95%).\n",
        "\n",
        "Caso o objetivo seja visualizar os dados, então devmos reduzir a dimensionalidade para 2 ou 3 dimensões.\n",
        "\n",
        "O código a seguir calcula o PCA sem reduzir a dimensionalidade e, em seguida, calcula o número mínimo de dimensões necessárias para preservar 95% da variância do conjunto de treinamento:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlrfRoC1mHvw"
      },
      "source": [
        "pca = PCA()\n",
        "pca.fit(X_treino)\n",
        "soma_cumulativa = np.cumsum(pca.explained_variance_ratio_)\n",
        "d = np.argmax(soma_cumulativa >= 0.95) + 1\n",
        "print('número de dimensões:', d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pdWVpmkXmHvx"
      },
      "source": [
        "É útil plotar a variância explicada como uma função do número de dimensões, criando um gráfico da soma cumulativa.\n",
        "\n",
        "Na curva normalmente haverá um trecho, usualmente chamada de \"cotovelo\" (ou _elbow_, em inglês) onde a variância explicada pára de crescer rapidamente. Esta inflexão pode ser interpretada como a dimensionalidade intrínseca deste conjunto de dados.\n",
        "\n",
        "Aqui, em particular, podemos ver que ao reduzirmos a dimensionalidade do PCA para cerca de 100 dimensões também não perderíamos muita da variância explicada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQl2QcRFmHvx"
      },
      "source": [
        "# plotagem da soma cumulativa das variâncias\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(soma_cumulativa, linewidth=3)\n",
        "plt.axis([0, 400, 0, 1])\n",
        "plt.xlabel('dimensões')\n",
        "plt.ylabel('variância explicada')\n",
        "plt.plot([d, d], [0, 0.95], 'k:')\n",
        "plt.plot([0, d], [0.95, 0.95], 'k:')\n",
        "plt.plot(d, 0.95, 'ko')\n",
        "plt.annotate('\"cotovelo\"', xy=(65, 0.85), xytext=(70, 0.7), arrowprops=dict(arrowstyle='->'), fontsize=16)\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U0TIDHNGmHvy"
      },
      "source": [
        "Uma vez definido o número de dimensões `d`, podemos fazer `n_components=d` e executar o PCA novamente.\n",
        "\n",
        "Porém, há uma opção muito melhor: em vez de especificar o número de componentes principais que desejamos preservar, podemos definir `n_components` como um valor real entre 0 e 1, indicando a proporção da variação que queremos manter.\n",
        "\n",
        "Então o número de componentes mantido é exatamente aquele que garante o percentual indicado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GOKGiG4JmHvz"
      },
      "source": [
        "pca = PCA(n_components=0.95)\n",
        "X_reduzido = pca.fit_transform(X_treino)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0D4y4EEjmHvz"
      },
      "source": [
        "# número de componentes selecionados\n",
        "pca.n_components_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0i6DujlHmHvz"
      },
      "source": [
        "# variação explicada total destes componentes\n",
        "np.sum(pca.explained_variance_ratio_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzl565l7mHvz"
      },
      "source": [
        "## Visualizando a perda de dados ao usar o PCA\n",
        "\n",
        "Após a redução da dimensionalidade, o conjunto de treinamento passa a ocupar muito menos espaço.\n",
        "\n",
        "No exemplo anterior o conjunto de dados MNIST teve a redução de 784 _features_ para apenas 153, ainda que preservando 95% de sua variação. Ou seja, o _dataset_ foi reduzido para cerca de 20% do seu tamanho original.\n",
        "\n",
        "Este é exatamente o princípio por trás da **compressão de dados com perda**: trocamos uma perda controlada da informação (que muitas vezes é redundante) por um conjunto mais compacto dos dados.\n",
        "\n",
        "Então vamos agora **descomprimir** o conjunto de dados reduzido de volta para 784 dimensões, aplicando a **transformação inversa** da projeção PCA. Isso é feito com a função `.inverse_transform()` sobre o conjunto de dados reduzido.\n",
        "\n",
        "Naturalmente não teremos de volta exatamente os dados originais, uma vez que a projeção perdeu um pouco de informação (dentro da variação de 5% que foi descartada), mas provavelmente uma boa aproximação deles."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7vYW5x2mHv0"
      },
      "source": [
        "X_recuperado = pca.inverse_transform(X_reduzido)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvyhcI5xmHv0"
      },
      "source": [
        "Vamos agora exibir um comparativo entre alguns dígitos do conjunto de treinamento original (à esquerda) e os dígitos correspondentes após a compressão e descompressão. Há uma ligeira perda de qualidade na imagem, mas os dígitos ainda estão intactos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkF0yoY9mHv1"
      },
      "source": [
        "# função auxiliar\n",
        "def plot_digits(instances, images_per_row=5, **options):\n",
        "    size = 28\n",
        "    images_per_row = min(len(instances), images_per_row)\n",
        "    images = [instance.reshape(size,size) for instance in instances]\n",
        "    n_rows = (len(instances) - 1) // images_per_row + 1\n",
        "    row_images = []\n",
        "    n_empty = n_rows * images_per_row - len(instances)\n",
        "    images.append(np.zeros((size, size * n_empty)))\n",
        "    for row in range(n_rows):\n",
        "        rimages = images[row * images_per_row : (row + 1) * images_per_row]\n",
        "        row_images.append(np.concatenate(rimages, axis=1))\n",
        "    image = np.concatenate(row_images, axis=0)\n",
        "    plt.imshow(image, cmap = 'binary', **options)\n",
        "    plt.axis('off')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2WK60UaUmHv1"
      },
      "source": [
        "# comparativo\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(121)\n",
        "plot_digits(X_treino[::2100])\n",
        "plt.title('original', fontsize=16)\n",
        "plt.subplot(122)\n",
        "plot_digits(X_recuperado[::2100])\n",
        "plt.title('recuperado', fontsize=16)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rzZJJ5UmHv3"
      },
      "source": [
        "A distância quadrada média entre os dados originais e os dados recuperados (comprimidos e depois descomprimidos) é chamada de **erro de reconstrução**.\n",
        "\n",
        "Isoladamente esta medida não faz muito sentido, pois depende da escala e da quantidade de atributos. Mas é uma métrica interessante para comparar diferentes aplicações do PCA."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0dvMjTZmHv3"
      },
      "source": [
        "loss = np.sum((X_treino - X_recuperado) ** 2, axis=1).mean()\n",
        "print('erro de reconstrução:', loss)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spQf-fvGmHv3"
      },
      "source": [
        "## Conjunto de dados \"Swiss roll\"\n",
        "\n",
        "Aqui vamos criar um conjunto mais complexo de dados, em que a abordagem linear no algoritmo PCA não é bem sucedida. Então precisamos de outros algoritmos para conseguie ter sucesso na redução da dimensionalidade deste _dataset_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZg3crLImHv4"
      },
      "source": [
        "from sklearn.datasets import make_swiss_roll\n",
        "\n",
        "X, t = make_swiss_roll(n_samples=1000, noise=0.2, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wk_02XMymHv4"
      },
      "source": [
        "# plotagem do dataset\n",
        "axes = [-11.5, 14, -2, 23, -12, 15]\n",
        "fig = plt.figure(figsize=(7, 7))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X[:, 0], X[:, 1], X[:, 2], c=t, cmap='hot')\n",
        "ax.view_init(10, -70)\n",
        "ax.set_xlabel(\"$x_1$\", fontsize=18)\n",
        "ax.set_ylabel(\"$x_2$\", fontsize=18)\n",
        "ax.set_zlabel(\"$x_3$\", fontsize=18)\n",
        "ax.set_xlim(axes[0:2])\n",
        "ax.set_ylim(axes[2:4])\n",
        "ax.set_zlim(axes[4:6])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4-YHTlYmHv4"
      },
      "source": [
        "## Algoritmo Kernel PCA\n",
        "\n",
        "O uso do algoritmo Kernel PCA é mais complexo, pois é preciso escolher um **kernel** e também **hiperparâmetros** para este ajuste.\n",
        "\n",
        "Além disso, não é mais possível usarmos a variância explicada como medida de controle para a projeção.\n",
        "\n",
        "Então não existe uma medida de desempenho óbvia para auxiliar na seleção dos melhores valores de _kernel_ e dos hiperparâmetros. No entanto, como a redução da dimensionalidade é muitas vezes uma etapa de preparação para uma tarefa de aprendizagem supervisionada (classificação ou regressão), é comum simplesmente usar a pesquisa em _grid_ testar uma certa quantidade de combinações de kernels e hiperparâmetros, mantendo os que levam ao melhor desempenho nessa tarefa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "psVwCnbymHv5"
      },
      "source": [
        "from sklearn.decomposition import KernelPCA\n",
        "\n",
        "rbf_pca = KernelPCA(n_components=2, kernel=\"rbf\", gamma=0.04, fit_inverse_transform=True)\n",
        "X_reduzido = rbf_pca.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iq-osYdmmHv5"
      },
      "source": [
        "Como a transformação usando Kernel PCA é não-linear, a transformação inversa não é imediata.\n",
        "\n",
        "Então, se esta tansformação for necessária, a opção `fit_inverse_transform=True` precisa ser passada na criação do objeto. Assim, podemos usar a mesma função `.inverse_transform()` para reconstruir os dados a partir do conjunto reduzido."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D235ilVgmHv5"
      },
      "source": [
        "X_recuperado = rbf_pca.inverse_transform(X_reduzido)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VcNQ1YeUmHv6"
      },
      "source": [
        "Para ilustrar, o código abaixo exibe o resultado da aplicação de três diferentes _kernels_ ao mesmo conjunto de dados `swissroll`, em que a redução é feita de três para duas dimensões."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uY9ESqXsmHv6"
      },
      "source": [
        "lin_pca = KernelPCA(n_components=2, kernel='linear')\n",
        "rbf_pca = KernelPCA(n_components=2, kernel='rbf', gamma=0.04)\n",
        "sig_pca = KernelPCA(n_components=2, kernel='sigmoid', gamma=0.001, coef0=1)\n",
        "\n",
        "plt.figure(figsize=(11, 3))\n",
        "for subplot, pca, title in ((131, lin_pca, 'kernel linear (PCA)'), \\\n",
        "                            (132, rbf_pca, 'kernel RBF'), \n",
        "                            (133, sig_pca, 'kernel sigmóide')):\n",
        "    X_reduzido = pca.fit_transform(X)    \n",
        "    plt.subplot(subplot)\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.scatter(X_reduzido[:, 0], X_reduzido[:, 1], c=t, cmap='hot')\n",
        "    plt.xlabel('$z_1$', fontsize=18)\n",
        "    if subplot == 131:\n",
        "        plt.ylabel('$z_2$', fontsize=18, rotation=0)\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfp2ZAowmHv6"
      },
      "source": [
        "\n",
        "# Abordagem usando aprendizado de variedades\n",
        "\n",
        "O algoritmo mais comum que usa aprendizado de variedades é o Incorporação Linear Local, ou em inglês, Locally Linear Embedding (LLE).\n",
        "\n",
        "\n",
        "## Algoritmo LLE\n",
        "\n",
        "Vamos demonstrar o algoritmo LLE sobre o mesmo conjunto de dados anterior, o `swissroll`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tWLg0291mHv6"
      },
      "source": [
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\n",
        "X_reduzido = lle.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S8lLfe_xmHv7"
      },
      "source": [
        "plt.figure(figsize=(5, 5))\n",
        "plt.title('LLE', fontsize=14)\n",
        "plt.scatter(X_reduzido[:, 0], X_reduzido[:, 1], c=t, cmap='hot')\n",
        "plt.xlabel('$z_1$', fontsize=18)\n",
        "plt.ylabel('$z_2$', fontsize=18)\n",
        "plt.axis([-0.065, 0.055, -0.1, 0.12])\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1NnQIk3SmHv7"
      },
      "source": [
        "# Outros algoritmos\n",
        "\n",
        "Aqui vamos apenas ilustrar brevemente outras técnicas de redução de dimensionalidade.\n",
        "\n",
        "O algoritmo **Multidimensional Scaling (MDS)** reduz a dimensionalidade enquanto tenta preservar as distâncias entre as instâncias.\n",
        "\n",
        "O algoritmo **Isomap** cria um gráfico conectando cada instância a seus vizinhos mais próximos e, em seguida, reduz a dimensionalidade enquanto tenta preservar as distâncias geodésicas entre as instâncias.\n",
        "\n",
        "O algoritmo **t-Distributed Stochastic Neighbor Embedding (t-SNE)** reduz a dimensionalidade ao tentar manter instâncias semelhantes próximas, ao mesmo tempo em que instâncias diferentes são separadas. É usado principalmente para visualização, em particular para exibir _clusters_ de instâncias em espaço de alta dimensão."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7LgMjc8cmHv7"
      },
      "source": [
        "from sklearn.manifold import MDS\n",
        "\n",
        "mds = MDS(n_components=2, random_state=42)\n",
        "X_reduzido_mds = mds.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rTmHmK_6mHv8"
      },
      "source": [
        "from sklearn.manifold import Isomap\n",
        "\n",
        "isomap = Isomap(n_components=2)\n",
        "X_reduzido_isomap = isomap.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKcKVimrmHv8"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "tsne = TSNE(n_components=2, random_state=42)\n",
        "X_reduzido_tsne = tsne.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LxcuJuuZmHv8"
      },
      "source": [
        "titles = ['MDS', 'Isomap', 't-SNE']\n",
        "datasets = (X_reduzido_mds, X_reduzido_isomap, X_reduzido_tsne)\n",
        "\n",
        "plt.figure(figsize=(11, 3))\n",
        "for subplot, title, X_reduzido in zip((131, 132, 133), titles, datasets):\n",
        "    plt.subplot(subplot)\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.scatter(X_reduzido[:, 0], X_reduzido[:, 1], c=t, cmap=plt.cm.hot)\n",
        "    plt.xlabel('$z_1$', fontsize=18)\n",
        "    if subplot == 131:\n",
        "        plt.ylabel('$z_2$', fontsize=18, rotation=0)\n",
        "    plt.grid(True)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjtsgdHEmHv9"
      },
      "source": [
        "Finalmente, a **Análise Discriminante Linear** ou **Linear Discriminant Analysis (LDA)** é na verdade um algoritmo de classificação. Porém, durante o treinamento este algoritmo aprende os eixos mais discriminativos entre as classes. Então tais eixos podem então ser usados para definir um hiperplano no qual projetar os dados. O benefício é que a projeção manterá as classes o mais distantes possível. Assim a LDA é uma boa técnica para reduzir a dimensionalidade antes de executar outro algoritmo de classificação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e_Lhj9MymHv9"
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "lda = LinearDiscriminantAnalysis(n_components=2)\n",
        "lda.fit(X_treino, y_treino) # observe aqui o uso necessário dos rótulos\n",
        "X_reduzido_lda = lda.transform(X_treino)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7Moj4-1mHv9"
      },
      "source": [
        "# Redução de tempo de treinamento\n",
        "\n",
        "Aqui vamos treinar um **classificador Random Forest** sobre o conjunto MNIST, cronometrando quanto tempo leva e avaliando o modelo resultante usando o conjunto de teste."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rSZv1lPzmHv9"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "\n",
        "%time rnd_clf.fit(X_treino, y_treino)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TmfROeOLmHv-"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "y_prev = rnd_clf.predict(X_teste)\n",
        "accuracy_score(y_teste, y_prev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIk9_4k_mHv_"
      },
      "source": [
        "Agora vamos usar o PCA para reduzir a dimensionalidade do conjunto de dados, usando uma proporção da variância explicada de 95%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XicsGFaJmHv_"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "X_treino_reduzido = pca.fit_transform(X_treino)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etAzbZ5vmHv_"
      },
      "source": [
        "Em seguida, vamos treinar um **novo classificador Random Forest** com o conjunto de dados reduzido e examinar quanto tempo leva."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m0admihRmHv_"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "rnd_clf2 = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "\n",
        "%time rnd_clf2.fit(X_treino_reduzido, y_treino)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOtH2XIMmHwA"
      },
      "source": [
        "O treino ficou **mais lento**. De fato, a redução da dimensionalidade nem sempre leva a um tempo de treinamento mais rápido: depende do conjunto de dados, do modelo e do algoritmo de treinamento.\n",
        "\n",
        "Agora vamos verificar a precisão do novo classificador."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wKZEdPgbmHwA"
      },
      "source": [
        "# note que o conjunto de teste também precisa ser reduzido\n",
        "X_teste_reduzido = pca.transform(X_teste)\n",
        "\n",
        "y_prev = rnd_clf2.predict(X_teste_reduzido)\n",
        "accuracy_score(y_teste, y_prev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezWK4pAHmHwB"
      },
      "source": [
        "É comum que o desempenho caia ligeiramente ao reduzir a dimensionalidade, porque perdemos algum sinal útil no processo. No entanto, a queda de desempenho é bastante severa neste caso. Portanto, o PCA realmente não ajudou: desacelerou o treinamento e reduziu o desempenho.\n",
        "\n",
        "Vamos agora testar os dados originais e reduzidos em um **classificador SGD**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-HhkaZD9mHwB"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf = SGDClassifier(random_state=42, loss='hinge')\n",
        "\n",
        "%time sgd_clf.fit(X_treino, y_treino)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeGiVpiNmHwB"
      },
      "source": [
        "y_prev = sgd_clf.predict(X_teste)\n",
        "accuracy_score(y_teste, y_prev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mB13jUtAmHwB"
      },
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "\n",
        "sgd_clf2 = SGDClassifier(random_state=42, loss='hinge')\n",
        "\n",
        "%time sgd_clf2.fit(X_treino_reduzido, y_treino)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-Hb7ADMmHwB"
      },
      "source": [
        "y_prev = sgd_clf2.predict(X_teste_reduzido)\n",
        "accuracy_score(y_teste, y_prev)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUHvT-wWmHwC"
      },
      "source": [
        "Para o classificados SGD tivemos uma ligeira queda no desempenho, que pode ser um preço razoável a pagar por um aumento significativo de velocidade de treinamento, dependendo da aplicação."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hLY_NmSfmHwC"
      },
      "source": [
        "# Uso de redução de dimensionalidade para visualização de dados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wogCPXtymHwC"
      },
      "source": [
        "Agora vamos usar diversos algoritmos para reduzir a dimensionalidade do conjunto MNIST para duas dimensões, de forma que possamos examinar visualmente o conjunto de dados.\n",
        "\n",
        "Para reduzir o tempo de processamento, vamos usar apenas 5,000 das 20,000 instâncias do _dataset_:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RaiKE6BBtH__"
      },
      "source": [
        "X = X_treino[:5000]\n",
        "y = y_treino[:5000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qctwEdzstISg"
      },
      "source": [
        "A ideia é usar a função `scatter()` da biblioteca Matplotlib para traçar um gráfico de dispersão, associando uma cor diferente para cada dígito. Para isso usamos uma função auxiliar, definida abaixo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcLOoTjemHwC"
      },
      "source": [
        "def simple_plot(X, y):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='jet')\n",
        "    plt.axis('off')\n",
        "    plt.colorbar()\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BQ53-r4amHwD"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "%time X_pca_reduzido = PCA(n_components=2, random_state=42).fit_transform(X)\n",
        "simple_plot(X_pca_reduzido, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FKJ-YgjmHwD"
      },
      "source": [
        "# ATENÇÃO: esse modelo demora um pouco, cerca de 1 minuto no Colaboratory\n",
        "\n",
        "from sklearn.manifold import LocallyLinearEmbedding\n",
        "\n",
        "%time X_lle_reduzido = LocallyLinearEmbedding(n_components=2, random_state=42).fit_transform(X)\n",
        "simple_plot(X_lle_reduzido, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6aiXmwEumHwE"
      },
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "%time X_lda_reduzido = LinearDiscriminantAnalysis(n_components=2).fit_transform(X, y)\n",
        "simple_plot(X_lda_reduzido, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iz-JEQC1mHwE"
      },
      "source": [
        "# ATENÇÃO: esse modelo demora um pouco, cerca de 2 minutos no Colaboratory\n",
        "\n",
        "from sklearn.manifold import TSNE\n",
        "\n",
        "%time X_tsne_reduzido = TSNE(n_components=2, random_state=42).fit_transform(X)\n",
        "simple_plot(X_tsne_reduzido, y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hocq3n5pmHwF"
      },
      "source": [
        "Pela proximidade entre os _clusters_ de cores, o último gráfico informa quais números são facilmente distinguíveis dos outros (por exemplo, 0, 6 e a maioria dos dígitos 8 são agrupamentos bem separados). Também diz quais números são muitas vezes difíceis de distinguir, como 4 e 9, ou os dígitos 5 e 3."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dc0wusdbmHwF"
      },
      "source": [
        "## Visualizações mais elaboradas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoY0TIqhmHwF"
      },
      "source": [
        "A função `plot_digits()` abaixo desenha um gráfico de dispersão e adiciona dígitos coloridos, com uma distância mínima garantida entre os mesmos. Se as imagens dos dígitos forem fornecidas, elas serão plotadas.\n",
        "\n",
        "Esta implementação foi inspirada em um dos excelentes exemplos do Scikit-Learn [plot_lle_digits](http://scikit-learn.org/stable/auto_examples/manifold/plot_lle_digits.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3O4UbvTmHwG"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from matplotlib.offsetbox import AnnotationBbox, OffsetImage\n",
        "\n",
        "def plot_digits(X, y, min_distance=0.05, images=None, figsize=(13, 10)):\n",
        "    # Let's scale the input features so that they range from 0 to 1\n",
        "    X_normalized = MinMaxScaler().fit_transform(X)\n",
        "    \n",
        "    # Now we create the list of coordinates of the digits plotted so far.\n",
        "    # We pretend that one is already plotted far away at the start, to\n",
        "    # avoid `if` statements in the loop below\n",
        "    neighbors = np.array([[10., 10.]])\n",
        "    \n",
        "    # The rest should be self-explanatory\n",
        "    plt.figure(figsize=figsize)\n",
        "    cmap = plt.cm.get_cmap(\"jet\")\n",
        "    digits = np.unique(y)\n",
        "    for digit in digits:\n",
        "        plt.scatter(X_normalized[y == digit, 0], X_normalized[y == digit, 1], c=[cmap(digit / 9)])\n",
        "    plt.axis(\"off\")\n",
        "    ax = plt.gcf().gca()  # get current axes in current figure\n",
        "    for index, image_coord in enumerate(X_normalized):\n",
        "        closest_distance = np.linalg.norm(neighbors - image_coord, axis=1).min()\n",
        "        if closest_distance > min_distance:\n",
        "            neighbors = np.r_[neighbors, [image_coord]]\n",
        "            if images is None:\n",
        "                plt.text(image_coord[0], image_coord[1], str(int(y[index])),\n",
        "                         color=cmap(y[index] / 9), fontdict={\"weight\": \"bold\", \"size\": 16})\n",
        "            else:\n",
        "                image = images[index].reshape(28, 28)\n",
        "                imagebox = AnnotationBbox(OffsetImage(image, cmap=\"binary\"), image_coord)\n",
        "                ax.add_artist(imagebox)\n",
        "                \n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KpYryvRxmHwH"
      },
      "source": [
        "# versão com dígitos coloridos\n",
        "plot_digits(X_tsne_reduzido, y, figsize=(30, 20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTugCcdWmHwI"
      },
      "source": [
        "# versão com imagens dos dígitos\n",
        "plot_digits(X_tsne_reduzido, y, figsize=(30, 20), images=X)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}