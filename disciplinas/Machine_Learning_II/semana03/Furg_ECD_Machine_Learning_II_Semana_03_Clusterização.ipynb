{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2"
    },
    "colab": {
      "name": "Furg - ECD - Machine Learning II - Semana 03 - Clusterização",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/atejap05/pos_data_science_furg/blob/main/disciplinas/Machine_Learning_II/semana03/Furg_ECD_Machine_Learning_II_Semana_03_Clusteriza%C3%A7%C3%A3o.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLuQFQf1fzkG"
      },
      "source": [
        "# Curso de Especialização em Ciência de Dados - FURG\n",
        "## Machine Learning I - Clusterização\n",
        "### Prof. Marcelo Malheiros\n",
        "\n",
        "Parte do código adaptada de Aurélien Geron (licença Apache-2.0)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LgsjMX9MfzkM"
      },
      "source": [
        "# Inicialização\n",
        "\n",
        "Aqui importamos as bibliotecas fundamentais de Python para este _notebook_:\n",
        "\n",
        "- NumPy: suporte a vetores, matrizes e operações de Álgebra Linear\n",
        "- Matplotlib: biblioteca de visualização de dados\n",
        "- Pandas: pacote estatístico e de manipulação de DataFrames\n",
        "- Scikit-Learn: biblioteca com algoritmos de Machine Learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6Y5CR01fzkN"
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import sklearn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sg764ExhfzkQ"
      },
      "source": [
        "# Comparativo de classificação versus clusterização\n",
        "\n",
        "Aqui vamos usar o tradicional _dataset_ IRIS para fazer a comparação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4HPh5qMfzkQ"
      },
      "source": [
        "from sklearn.datasets import load_iris"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO90MrIifzkR"
      },
      "source": [
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "print(data.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yTUXq5rfzkT"
      },
      "source": [
        "Visualmente, a diferença entre as duas tarefas pode ser ilustrada pelos dois gráficos abaixo, distinguindo as classes (à esquerda) e sem distinguir (à direita)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HAF8Me28fzkT"
      },
      "source": [
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(121)\n",
        "plt.plot(X[y==0, 2], X[y==0, 3], 'yo', label='Iris setosa')\n",
        "plt.plot(X[y==1, 2], X[y==1, 3], 'bs', label='Iris versicolor')\n",
        "plt.plot(X[y==2, 2], X[y==2, 3], 'g^', label='Iris virginica')\n",
        "plt.xlabel('Comprimento da pétala', fontsize=14)\n",
        "plt.ylabel('Largura da pétala', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.subplot(122)\n",
        "plt.scatter(X[:, 2], X[:, 3], c='k', marker='.')\n",
        "plt.xlabel('Comprimento da pétala', fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Iy4JBwXUfzkU"
      },
      "source": [
        "Usar um algoritmo de agrupamento da biblioteca Scikit-Learn segue o mesmo conjunto de passos de outros algoritmos: importar o algoritmo, criar um objeto (especificando os hiperparâmetros mais importantes) e fazer o processamento."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q82ttPfnfzkW"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(random_state=42, n_clusters=3)\n",
        "\n",
        "y_pred = kmeans.fit_predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO2uXZfRfzkX"
      },
      "source": [
        "O algoritmo `KMeans` encontrou os três _clusters_ solicitados, que vamos colorizar manualmente com as mesmas cores dos rótulos originais, para efeito de comparação."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIFcQ4xBfzkX"
      },
      "source": [
        "mapping = [1, 0, 2]\n",
        "y_map = np.array([mapping[cluster_id] for cluster_id in y_pred])\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(121)\n",
        "plt.plot(X[y==0, 2], X[y==0, 3], 'yo', label='Iris setosa')\n",
        "plt.plot(X[y==1, 2], X[y==1, 3], 'bs', label='Iris versicolor')\n",
        "plt.plot(X[y==2, 2], X[y==2, 3], 'g^', label='Iris virginica')\n",
        "plt.xlabel('Comprimento da pétala', fontsize=14)\n",
        "plt.ylabel('Largura da pétala', fontsize=14)\n",
        "plt.legend(fontsize=12)\n",
        "plt.subplot(122)\n",
        "plt.plot(X[y_map==0, 2], X[y_map==0, 3], 'yo', label='cluster 0')\n",
        "plt.plot(X[y_map==1, 2], X[y_map==1, 3], 'bs', label='cluster 2')\n",
        "plt.plot(X[y_map==2, 2], X[y_map==2, 3], 'g^', label='cluster 3')\n",
        "plt.xlabel('Comprimento da pétala', fontsize=14)\n",
        "plt.legend(loc='upper left', fontsize=12)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QW9SX91vfzkY"
      },
      "source": [
        "# percentual de acertos\n",
        "print('{:.2%}'.format(np.sum(y_map==y) / len(y)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gEWdpLLfzkZ"
      },
      "source": [
        "# Algoritmo K-Means\n",
        "\n",
        "Inicialmente vamos construir um _dataset_ sintético com \"nuvens\" de pontos, usando a função `make_blobs`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dXeDU0AhfzkZ"
      },
      "source": [
        "from sklearn.datasets import make_blobs\n",
        "\n",
        "# centro de cada nuvem\n",
        "blob_centers = np.array([[ 0.2, 2.3], [-1.5, 2.3], [-2.8, 1.8], [-2.8, 2.8], [-2.8, 1.3]])\n",
        "\n",
        "# dispersão de cada nuvem\n",
        "blob_std = np.array([0.4, 0.3, 0.1, 0.1, 0.1])\n",
        "\n",
        "# criação dos dados\n",
        "X, y = make_blobs(n_samples=2000, centers=blob_centers, cluster_std=blob_std, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4MTRy_Fzfzka"
      },
      "source": [
        "# função auxiliar\n",
        "def plot_clusters(X, y=None):\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=y, s=1)\n",
        "    plt.xlabel(\"$x_1$\", fontsize=14)\n",
        "    plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O2lpzNiwfzka"
      },
      "source": [
        "# gráfico de dispersão\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_clusters(X)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4HqGJxoUfzkb"
      },
      "source": [
        "## Ajuste e predição\n",
        "\n",
        "Vamos aplicar o algoritmo K-Means para este conjunto de dados. Ele tentará encontrar o centro de cada nuvem e atribuir cada instância ao centróide mais próximo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kzpQZqzcfzkc"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k = 5\n",
        "kmeans = KMeans(random_state=42, n_clusters=k)\n",
        "y_pred = kmeans.fit_predict(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rNmTOc3Cfzkc"
      },
      "source": [
        "Cada instância foi associada a um dos cinco _clusters_, de números 0 até 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVFVkmZ-fzkc"
      },
      "source": [
        "print(y_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIlMFUYtfzkd"
      },
      "source": [
        "Depois de feito o ajuste, os _labels_ também podem ser acessados via campo `.labels_`. Os centróides (ou seja, os centros dos _clusters_) calculados estão acessíveis via `.cluster_centers_`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oqtkv7hifzkd"
      },
      "source": [
        "print('rótulos:', kmeans.labels_)\n",
        "print('centróides:\\n', kmeans.cluster_centers_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0VYfzHcfzkd"
      },
      "source": [
        "Agora podemos predizer os rótulos de novas instâncias:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRKmt6H8fzke"
      },
      "source": [
        "X_new = np.array([[0, 2], [3, 2], [-3, 3], [-3, 2]])\n",
        "print('rótulos preditos:', kmeans.predict(X_new))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNTz93Plfzke"
      },
      "source": [
        "## Fronteiras de decisão\n",
        "\n",
        "Ao fazer o agrupamento em regiões, o algoritmo K-Means estabelece **fronteiras de decisão**. Vamos visualizar elas a seguir, usando uma subdivisão espacial chamada **diagrama de Voronoi**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T-hodO_Wfzkf"
      },
      "source": [
        "# funções auxiliares de desenho\n",
        "\n",
        "def plot_data(X):\n",
        "    plt.plot(X[:, 0], X[:, 1], 'k.', markersize=2)\n",
        "\n",
        "def plot_centroids(centroids, weights=None, circle_color='w', cross_color='k'):\n",
        "    if weights is not None:\n",
        "        centroids = centroids[weights > weights.max() / 10]\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1], \n",
        "                marker='o', s=35, linewidths=8,\n",
        "                color=circle_color, zorder=10, alpha=0.9)\n",
        "    plt.scatter(centroids[:, 0], centroids[:, 1],\n",
        "                marker='x', s=2, linewidths=12,\n",
        "                color=cross_color, zorder=11, alpha=1)\n",
        "\n",
        "def plot_decision_boundaries(clusterer, X, resolution=1000, show_centroids=True,\n",
        "                             show_xlabels=True, show_ylabels=True):\n",
        "    mins = X.min(axis=0) - 0.1\n",
        "    maxs = X.max(axis=0) + 0.1\n",
        "    xx, yy = np.meshgrid(np.linspace(mins[0], maxs[0], resolution),\n",
        "                         np.linspace(mins[1], maxs[1], resolution))\n",
        "    Z = clusterer.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "\n",
        "    plt.contourf(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), cmap=\"Pastel2\")\n",
        "    plt.contour(Z, extent=(mins[0], maxs[0], mins[1], maxs[1]), linewidths=1, colors='k')\n",
        "    plot_data(X)\n",
        "    if show_centroids:\n",
        "        plot_centroids(clusterer.cluster_centers_)\n",
        "\n",
        "    if show_xlabels:\n",
        "        plt.xlabel(\"$x_1$\", fontsize=14)\n",
        "    else:\n",
        "        plt.tick_params(labelbottom=False)\n",
        "    if show_ylabels:\n",
        "        plt.ylabel(\"$x_2$\", fontsize=14, rotation=0)\n",
        "    else:\n",
        "        plt.tick_params(labelleft=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2ubThNvfzkf"
      },
      "source": [
        "plt.figure(figsize=(8, 4))\n",
        "plot_decision_boundaries(kmeans, X)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u-3euWD1fzkg"
      },
      "source": [
        "Note que algumas das instâncias próximas às bordas provavelmente foram atribuídas ao _cluster_ errado. Mesmo assim, o algoritmo foi bem-sucedido."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XU9NPGmffzkg"
      },
      "source": [
        "## Hard clustering versus soft clustering\n",
        "\n",
        "Em vez de escolher somente o _cluster_ mais próximo para cada instância, o que é chamado de _hard clustering_, podemode medir a distância de cada instância a cada um dos 5 centróides. Isso é o que o método `transform()` faz."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntlfoenAfzkg"
      },
      "source": [
        "kmeans.transform(X_new)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpufkqXlfzkh"
      },
      "source": [
        "Estas diversas distãncias podem então ser combinadas em uma medida ponderada, um _score_, que pode então indicar melhor a pertinência ou não de cada instãncia a um _cluster_. Esse é o princípio do _soft clustering_.\n",
        "\n",
        "Note que esta também pode ser encarada como uma técnica de **redução de dimensionalidade**, caso o número de atributos seja maior que o número de _clusters_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "og3V-_eufzki"
      },
      "source": [
        "## Funcionamento do algoritmo original K-Means\n",
        "\n",
        "O algoritmo K-Means é um dos algoritmos de clusterização mais rápidos e também um dos mais simples. A versão original dele, que tem algumas limitações, funciona assim:\n",
        "\n",
        "- Inicialize $k$ centróides com posições aleatórias, copiadas de $k$ instâncias distintas escolhidas randomicamente.\n",
        "\n",
        "- Repita até a convergência (ou seja, até que os centróides parem de se mover):\n",
        "\n",
        "     - Atribua cada instância ao centróide mais próximo.\n",
        "     \n",
        "     - Calcule a média das posições de todas as instâncias de mesma classe.\n",
        "     \n",
        "     - Atualize cada centróide com sua respectiva média de posições.\n",
        "     \n",
        "A seguir, uma breve ilustração do processo é feita usando a própria classe `KMeans`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c0XOIrVqfzki"
      },
      "source": [
        "# três iterações do algoritmo\n",
        "kmeans_iter1 = KMeans(n_clusters=5, init='random', n_init=1, algorithm='full', max_iter=1, random_state=7)\n",
        "kmeans_iter2 = KMeans(n_clusters=5, init='random', n_init=1, algorithm='full', max_iter=2, random_state=7)\n",
        "kmeans_iter3 = KMeans(n_clusters=5, init='random', n_init=1, algorithm='full', max_iter=3, random_state=7)\n",
        "kmeans_iter1.fit(X)\n",
        "kmeans_iter2.fit(X)\n",
        "kmeans_iter3.fit(X)\n",
        "\n",
        "# plotagem\n",
        "plt.figure(figsize=(10, 8))\n",
        "plt.subplot(321)\n",
        "plot_data(X)\n",
        "plot_centroids(kmeans_iter1.cluster_centers_, circle_color='r', cross_color='w')\n",
        "plt.ylabel('$x_2$', fontsize=14, rotation=0)\n",
        "plt.tick_params(labelbottom=False)\n",
        "plt.title('Atualização dos centróides', fontsize=14)\n",
        "plt.subplot(322)\n",
        "plot_decision_boundaries(kmeans_iter1, X, show_xlabels=False, show_ylabels=False)\n",
        "plt.title('Rotulação das instâncis', fontsize=14)\n",
        "plt.subplot(323)\n",
        "plot_decision_boundaries(kmeans_iter1, X, show_centroids=False, show_xlabels=False)\n",
        "plot_centroids(kmeans_iter2.cluster_centers_)\n",
        "plt.subplot(324)\n",
        "plot_decision_boundaries(kmeans_iter2, X, show_xlabels=False, show_ylabels=False)\n",
        "plt.subplot(325)\n",
        "plot_decision_boundaries(kmeans_iter2, X, show_centroids=False)\n",
        "plot_centroids(kmeans_iter3.cluster_centers_)\n",
        "plt.subplot(326)\n",
        "plot_decision_boundaries(kmeans_iter3, X, show_ylabels=False)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1C8qkzdYfzkj"
      },
      "source": [
        "Na versão original do algoritmo K-Means, um grande problema é que se você executar ele várias vezes (ou com diferentes sementes aleatórias), a tendência é convergir para soluções muito diferentes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FuvYjSrufzkj"
      },
      "source": [
        "def plot_clusterer_comparison(clusterer1, clusterer2, X, title1=None, title2=None):\n",
        "    clusterer1.fit(X)\n",
        "    clusterer2.fit(X)\n",
        "    plt.figure(figsize=(10, 3.2))\n",
        "    plt.subplot(121)\n",
        "    plot_decision_boundaries(clusterer1, X)\n",
        "    if title1:\n",
        "        plt.title(title1, fontsize=14)\n",
        "    plt.subplot(122)\n",
        "    plot_decision_boundaries(clusterer2, X, show_ylabels=False)\n",
        "    if title2:\n",
        "        plt.title(title2, fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVPbJQE3fzkk"
      },
      "source": [
        "kmeans_rnd_init1 = KMeans(n_clusters=5, init='random', n_init=1, algorithm='full', random_state=0)\n",
        "kmeans_rnd_init2 = KMeans(n_clusters=5, init='random', n_init=1, algorithm='full', random_state=3)\n",
        "plot_clusterer_comparison(kmeans_rnd_init1, kmeans_rnd_init2, X, 'solução ruim 1', 'solução ruim 2')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toAtQxwPfzkk"
      },
      "source": [
        "## Uso da métrica de inércia\n",
        "\n",
        "Se necessário, poderíamos rodar o K-Means diversas vezes e utilizar a métrica de **inércia** para comparar os resultados, pegando o melhor modelo\n",
        "\n",
        "A inércia é a soma das distâncias quadradas entre cada instância de treinamento e seu centróide mais próximo, e pode ser obtida diretamente após o treinamento via campo `.inertia_`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOQlzuzyfzkk"
      },
      "source": [
        "kmeans.inertia_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tF4iZ-Xffzkl"
      },
      "source": [
        "O método `score()` retorna a inércia negativa, pois o método `score()` de um preditor deve sempre respeitar a regra de que _\"maior é melhor\"_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YjEd_j-Cfzkl"
      },
      "source": [
        "kmeans.score(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Txtfv5Hcfzkl"
      },
      "source": [
        "Quando definimos o hiperparâmetro `n_init`, o Scikit-Learn executa o algoritmo original `n_init` vezes, selecionando a solução que minimiza a inércia. Por padrão, o Scikit-Learn define `n_init = 10`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_B8_XqkNfzkl"
      },
      "source": [
        "kmeans_rnd_10_inits = KMeans(n_clusters=5, init='random', n_init=10, algorithm='full', random_state=2)\n",
        "kmeans_rnd_10_inits.fit(X)\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plot_decision_boundaries(kmeans_rnd_10_inits, X)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3wUTT1_fzkm"
      },
      "source": [
        "## Otimizações do K-Means\n",
        "\n",
        "Na prática, a **configuração padrão** do `K-Means` já incorpora diversas otimizações em relação ao algoritmo original, de forma que usualmente não é necessário alterar os hiperparâmetros `init`, `n_init` ou `algorithm`.\n",
        "\n",
        "Resumidamente essa otimizações já escolhem o melhor modelo entre diversos (usando a inércia como métrica), incorporam uma estratégia de melhor escolha de centróides iniciais (K-Means++) e reduz o tempo de cálculo das distâncias (Accelerated K-Means)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mbnFMewfzkn"
      },
      "source": [
        "X_large, y_large = make_blobs(n_samples=1000000, centers=blob_centers, cluster_std=blob_std, random_state=42)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJ-mfOOxfzkn"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "k = 5\n",
        "df_kmeans = KMeans(random_state=421, n_clusters=k)\n",
        "%time df_kmeans.fit(X_large)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pyqd7d8c3vHT"
      },
      "source": [
        "Em vez de usar o conjunto de dados completo em cada iteração, o algoritmo\n",
        "'MiniBatchKMeans` é capaz de usar lotes de instâncias, movendo os centróides apenas ligeiramente a cada iteração. Isso acelera o algoritmo normalmente por um fator de três ou quatro e torna possível clusterizar grandes conjuntos de dados, mesmo que não caibam na memória.\n",
        "\n",
        "Aqui vamos fazer o agrupamento com o mesmo conjunto de dados anterior, para ver que há um ganho de tempo na execução."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCKQOifpfzkn"
      },
      "source": [
        "from sklearn.cluster import MiniBatchKMeans\n",
        "\n",
        "mb_kmeans = MiniBatchKMeans(n_clusters=5, random_state=42)\n",
        "%time mb_kmeans.fit(X_large)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P66fg_wQfzkn"
      },
      "source": [
        "## Encontrando o número ótimo de clusters\n",
        "\n",
        "O hiperparâmetro mais importante (e normalmente o único) a ser definido é $k$, a quantidade de _clusters_ desejada.\n",
        "\n",
        "Essa escolha é extremamente importante, pois deve corresponder aproximadamente à organização (ainda não completamente conhecida) dos dados. Um diagrama de dispersão pode ajudar a visualizar a relação entre duas ou três _features_, mas ainda assim é limitada para sugerir o número ideal de _clusters_.\n",
        "\n",
        "Um número pequeno ou elevado tem efeitos ruins sobre o processo de agrupamento, como mostram as figuras a seguir."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW9iEFR8fzko"
      },
      "source": [
        "kmeans_k3 = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans_k8 = KMeans(n_clusters=8, random_state=42)\n",
        "plot_clusterer_comparison(kmeans_k3, kmeans_k8, X, '$k=3$', '$k=8$')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9_XUiAofzkp"
      },
      "source": [
        "kmeans_k3.inertia_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pesrnahufzkp"
      },
      "source": [
        "kmeans_k8.inertia_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcFkps6nfzkp"
      },
      "source": [
        "Neste caso não é possível tomar o valor de $k$ que minimiza a inércia, já que este fica cada vez menor à medida que aumentamos $k$.\n",
        "\n",
        "De fato, quanto mais _clusters_ houver, mais perto cada instância estará de seu respectivo centróide e, portanto, menor será a inércia.\n",
        "\n",
        "Devemos então plotar a inércia em função de $k$ e analisar o \"cotovelo\" ou _elbow_ da curva resultante. Esse ponto indicará um valor aproximado (para mais ou para menos) em que devemos considerar a escolha do número de _clusters_."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_zm6hF4fzkq"
      },
      "source": [
        "# gera diversos modelos à medida em que k varia\n",
        "kmeans_per_k = [KMeans(n_clusters=k, random_state=42).fit(X) for k in range(1, 10)]\n",
        "inertias = [model.inertia_ for model in kmeans_per_k]\n",
        "\n",
        "# gráfico\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(1, 10), inertias, 'bo-')\n",
        "plt.xlabel('$k$', fontsize=14)\n",
        "plt.ylabel('inércia', fontsize=14)\n",
        "plt.annotate('cotovelo', xy=(4, inertias[3]), xytext=(0.55, 0.55), textcoords='figure fraction',\n",
        "             fontsize=16, arrowprops=dict(facecolor='black', shrink=0.1))\n",
        "plt.axis([1, 8.5, 0, 1300])\n",
        "plt.savefig('inertia-k', dpi=100)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6isjhPtfzkq"
      },
      "source": [
        "Outra abordagem é olhar para a **pontuação da silhueta** (ou _silhouette score_). Este valor é calculado como o coeficiente da silhueta médio de todas as instâncias.\n",
        "\n",
        "O **coeficiente de silhueta** de uma instância é igual a $(b - a) /\\ max(a, b)$, onde $a$ é a distância média para as outras instâncias no mesmo _cluster_ e $b$ é a distância média do _cluster_ mais próximo.\n",
        "\n",
        "Cada coeficiente de silhueta pode variar entre -1 e +1: um coeficiente próximo a +1 significa que a instância está bem dentro de seu próprio _cluster_ e longe de outros _clusters_, enquanto um coeficiente próximo a 0 significa que está perto do limite de um _cluster_. Finalmente, um coeficiente próximo a -1 significa que a instância pode ter sido atribuída ao _cluster_ errado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUHOXm9cfzkr"
      },
      "source": [
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "silhouette_score(X, kmeans.labels_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8IQ_IbQfzkr"
      },
      "source": [
        "Podemos então plotar a variação do _silhouette score_ à medida em que $k$ varia."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f_9kCZN4fzkr"
      },
      "source": [
        "silhouette_scores = [silhouette_score(X, model.labels_) for model in kmeans_per_k[1:]]\n",
        "\n",
        "plt.figure(figsize=(8, 4))\n",
        "plt.plot(range(2, 10), silhouette_scores, 'bo-')\n",
        "plt.xlabel('$k$', fontsize=14)\n",
        "plt.ylabel('silhouette score', fontsize=14)\n",
        "plt.axis([1.8, 8.5, 0.55, 0.7])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Klf99YPfzks"
      },
      "source": [
        "De fato, esta visualização é muito mais rica do que a anterior. Embora o gráfico confirme que $k = 4$ é uma escolha muito boa, também destaca o fato de que $k = 5$ também é outra ótima opção.\n",
        "\n",
        "Uma visualização ainda mais informativa é fornecida quando plotamos o coeficiente de silhueta de cada instância, classificado pelo _cluster_ ao qual eles são atribuídos e pelo valor deste coeficiente. Isso é chamado de **diagrama de silhueta**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FXvcEwyLfzks"
      },
      "source": [
        "from sklearn.metrics import silhouette_samples\n",
        "from matplotlib.ticker import FixedLocator, FixedFormatter\n",
        "\n",
        "plt.figure(figsize=(11, 9))\n",
        "for k in (3, 4, 5, 6):\n",
        "    plt.subplot(2, 2, k - 2)\n",
        "    y_pred = kmeans_per_k[k - 1].labels_\n",
        "    silhouette_coefficients = silhouette_samples(X, y_pred)\n",
        "    padding = len(X) // 30\n",
        "    pos = padding\n",
        "    ticks = []\n",
        "    for i in range(k):\n",
        "        coeffs = silhouette_coefficients[y_pred == i]\n",
        "        coeffs.sort()\n",
        "        color = plt.cm.Spectral(i / k)\n",
        "        plt.fill_betweenx(np.arange(pos, pos + len(coeffs)), 0, coeffs, facecolor=color, edgecolor=color, alpha=0.7)\n",
        "        ticks.append(pos + len(coeffs) // 2)\n",
        "        pos += len(coeffs) + padding\n",
        "    plt.gca().yaxis.set_major_locator(FixedLocator(ticks))\n",
        "    plt.gca().yaxis.set_major_formatter(FixedFormatter(range(k)))\n",
        "    if k in (3, 5):\n",
        "        plt.ylabel('cluster')\n",
        "    if k in (5, 6):\n",
        "        plt.gca().set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
        "        plt.xlabel('silhouette coefficient')\n",
        "    else:\n",
        "        plt.tick_params(labelbottom=False)\n",
        "    plt.axvline(x=silhouette_scores[k - 2], color='red', linestyle='--')\n",
        "    plt.title('$k={}$'.format(k), fontsize=16)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vvf3oLm9fzkt"
      },
      "source": [
        "Cada diagrama contém uma forma de \"faca\" por _cluster_. A altura da forma indica o número de instâncias que o _cluster_ contém, e sua largura representa os coeficientes de silhueta ordenados (das instâncias deste _cluster_). Então quanto mais comprida a \"faca\", melhor. A linha tracejada indica o coeficiente de silhueta médio.\n",
        "\n",
        "Podemos ver que $k = 5$ parece a melhor opção, já que todos os _clusters_ são aproximadamente do mesmo tamanho e todos eles cruzam a linha tracejada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9-bI4pTfzkt"
      },
      "source": [
        "## Limitações do K-Means\n",
        "\n",
        "O K-Means não se comporta muito bem quando os _clusters_ têm tamanhos variados, densidades diferentes ou formas achatadas. Nesse caso outros algoritmos devem ser experimentados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4YDbqxJfzkt"
      },
      "source": [
        "# dataset problemático\n",
        "\n",
        "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
        "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
        "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
        "X2 = X2 + [6, -8]\n",
        "X = np.r_[X1, X2]\n",
        "y = np.r_[y1, y2]\n",
        "\n",
        "plot_clusters(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGh8Laqyfzku"
      },
      "source": [
        "# modelo bem sucedido (esquerda)\n",
        "kmeans_good = KMeans(n_clusters=3, init=np.array([[-1.5, 2.5], [0.5, 0], [4, 0]]), n_init=1, random_state=42)\n",
        "kmeans_good.fit(X)\n",
        "\n",
        "# modelo mal sucedido (direita)\n",
        "kmeans_bad = KMeans(n_clusters=3, random_state=42)\n",
        "kmeans_bad.fit(X)\n",
        "\n",
        "# figuras\n",
        "plt.figure(figsize=(10, 3.2))\n",
        "plt.subplot(121)\n",
        "plot_decision_boundaries(kmeans_good, X)\n",
        "plt.title('inércia = {:.1f}'.format(kmeans_good.inertia_), fontsize=14)\n",
        "plt.subplot(122)\n",
        "plot_decision_boundaries(kmeans_bad, X, show_ylabels=False)\n",
        "plt.title('inércia = {:.1f}'.format(kmeans_bad.inertia_), fontsize=14)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx2yuNc-fzku"
      },
      "source": [
        "# Algoritmo DBSCAN\n",
        "\n",
        "Da mesma forma que para o K-Means, a biblioteca Scikit-Learn disponibiliza um algoritmo `DBSCAN` pronto para o uso, e seguindo a mesma interface de programação.\n",
        "\n",
        "Aqui vamos usar como teste o _dataset_ sintético `moons`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uMfGvD6fzku"
      },
      "source": [
        "from sklearn.datasets import make_moons\n",
        "\n",
        "X, y = make_moons(n_samples=1000, noise=0.05, random_state=42)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.scatter(X[:,0], X[:,1])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QRtFuTOlfzkv"
      },
      "source": [
        "Para o `DBSCAN`, precisamos obrigatoriamente informar dois hiperparâmetros: `eps`, que indica uma distância pequena para definir a **proximidade** entre instâncias, e `min_samples`, que é o número mínimo de instâncias próximas para termos uma **região densa**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRR7MKqWfzkv"
      },
      "source": [
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "dbscan = DBSCAN(eps=0.05, min_samples=5)\n",
        "dbscan.fit(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9a_5XcOfzkv"
      },
      "source": [
        "O algoritmo DBSCAN define _clusters_ como regiões contínuas de alta densidade. Funciona assim:\n",
        "\n",
        "- Para cada instância, o algoritmo conta quantas instâncias estão localizadas a uma pequena distância `eps` dela. Esta região é chamada de vizinhança da instância.\n",
        "\n",
        "- Se uma instância tem pelo menos `min_samples` instâncias em sua vizinhança (incluindo ela mesma), então ela é considerada uma **instância central**. Ou seja, as instâncias centrais são aquelas localizadas em regiões densas.\n",
        "\n",
        "- Todas as instâncias na vizinhança de uma instância central pertencem ao mesmo _cluster_. Essa vizinhança pode incluir outras instâncias centrais. Dessa maneira uma longa sequência de instâncias centrais vizinhas forma um único _cluster_.\n",
        "\n",
        "- Qualquer instância que não seja uma instância central e que também não tenha uma instância central em sua vizinhança é considerada uma **anomalia**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PcjgsUmkfzkw"
      },
      "source": [
        "print('dez primeiros rótulos:', dbscan.labels_[:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zwM-yTovfzkw"
      },
      "source": [
        "Observe que algumas instâncias têm um índice de cluster igual a `-1`, o que significa que eles são considerados como anomalias pelo algoritmo.\n",
        "\n",
        "Os índices das instâncias centrais são disponível na variável `.core_sample_indices_`, e os próprios atributos estão disponíveis na variável `.components_`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swsAn8Frfzkx"
      },
      "source": [
        "print('número de instâncias centrais:', len(dbscan.core_sample_indices_))\n",
        "print('cinco primeiros índices das instâncias centrais:', dbscan.core_sample_indices_[:5])\n",
        "print('cinco primeiras instâncias centrais:\\n', dbscan.components_[:5])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNeCZLXwfzkx"
      },
      "source": [
        "print('rótulos definidos:', np.unique(dbscan.labels_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XPajNN8kfzky"
      },
      "source": [
        "# função auxiliar\n",
        "def plot_dbscan(dbscan, X, size, show_xlabels=True, show_ylabels=True):\n",
        "    core_mask = np.zeros_like(dbscan.labels_, dtype=bool)\n",
        "    core_mask[dbscan.core_sample_indices_] = True\n",
        "    anomalies_mask = dbscan.labels_ == -1\n",
        "    non_core_mask = ~(core_mask | anomalies_mask)\n",
        "    cores = dbscan.components_\n",
        "    anomalies = X[anomalies_mask]\n",
        "    non_cores = X[non_core_mask]\n",
        "    plt.scatter(cores[:, 0], cores[:, 1], c=dbscan.labels_[core_mask], marker='o', s=size, cmap='Paired')\n",
        "    plt.scatter(cores[:, 0], cores[:, 1], marker='o', s=20, c=dbscan.labels_[core_mask])\n",
        "    plt.scatter(anomalies[:, 0], anomalies[:, 1], c='r', marker='x', s=100)\n",
        "    plt.scatter(non_cores[:, 0], non_cores[:, 1], c=dbscan.labels_[non_core_mask], marker='.')\n",
        "    if show_xlabels:\n",
        "        plt.xlabel('$x_1$', fontsize=14)\n",
        "    else:\n",
        "        plt.tick_params(labelbottom=False)\n",
        "    if show_ylabels:\n",
        "        plt.ylabel('$x_2$', fontsize=14, rotation=0)\n",
        "    else:\n",
        "        plt.tick_params(labelleft=False)\n",
        "    plt.title('eps={:.2f}, min_samples={}'.format(dbscan.eps, dbscan.min_samples), fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF2haURhfzky"
      },
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plot_dbscan(dbscan, X, size=100)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zlf1J1dhfzkz"
      },
      "source": [
        "O DBSCAN resultou no agrupamento mostrado no gráfico anterior.\n",
        "\n",
        "O algoritmo identificou muitas anomalias, indicadas em vermelho, além de sete _clusters_ diferentes. Isso aconteceu por conta do pequeno valor de `eps` definido.\n",
        "\n",
        "Se alargarmos a vizinhança de cada instância aumentando `eps` para `0.2`, obtemos o agrupamento abaixo, que se ajusta muito bem aos dados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FNL5_xBUfzkz"
      },
      "source": [
        "# novo modelo\n",
        "dbscan = DBSCAN(eps=0.2, min_samples=5)\n",
        "dbscan.fit(X)\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plot_dbscan(dbscan, X, size=600)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkQuTvs5fzkz"
      },
      "source": [
        "## Predição com o DBSCAN\n",
        "\n",
        "Note que a classe DBSCAN não tem um método `predict()` para predizer novas instâncias. O algoritmo faz apenas o agrupamento do conjunto de dados fornecido.\n",
        "\n",
        "Contudo, é bem direto usar agora um classificador tradicional, como por exemplo o algoritmo `KNeighborsClassifier`, para aprender a estrutura dos dados e ser capaz de classificar novas instâncias.\n",
        "\n",
        "É isso o que o código abaixo faz, usando como treino da classificação apenas as instâncias centrais. Outras estratégias são possíveis, usando ou não todas as instâncias, ou mesmo usando também as anomalias."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNNeB0DSfzk0"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "# criação do classificador e treinamento com as instâncias centrais apenas\n",
        "knn = KNeighborsClassifier(n_neighbors=50)\n",
        "knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])\n",
        "\n",
        "# predição de quatro novas instâncias\n",
        "X_new = np.array([[-0.5, 0], [0, 0.5], [1, -0.1], [2, 1]])\n",
        "print(knn.predict(X_new))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-TWEZ7rfzk0"
      },
      "source": [
        "# probabilidades das novas instâncias pertencerem a cada um dos dois clusters\n",
        "print(knn.predict_proba(X_new))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "newnB2OTfzk1"
      },
      "source": [
        "# visualização da fronteira de decisão do classificador\n",
        "plt.figure(figsize=(8, 5))\n",
        "plot_decision_boundaries(knn, X, show_centroids=False)\n",
        "plt.scatter(X_new[:, 0], X_new[:, 1], c='r', marker='+', s=200, zorder=10)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M5CbHEWfzk1"
      },
      "source": [
        "# anomalias podem ser detectadas impondo uma distância máxima dos vizinhos\n",
        "y_dist, y_pred_idx = knn.kneighbors(X_new, n_neighbors=1)\n",
        "y_pred = dbscan.labels_[dbscan.core_sample_indices_][y_pred_idx]\n",
        "\n",
        "# aqui a distânia máxima é 0.2: anomalias são marcadas com -1\n",
        "y_pred[y_dist > 0.2] = -1\n",
        "y_pred.ravel()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuvvm3Gwfzk1"
      },
      "source": [
        "## Limitações do DBSCAN\n",
        "\n",
        "O DBSCAN não se comporta muito bem quando os _clusters_ não estão suficientemente separados por regiões de baixa densidade. Nesse caso outros algoritmos devem ser experimentados."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jCseUaQ9fzk2"
      },
      "source": [
        "# dataset problemático\n",
        "\n",
        "X1, y1 = make_blobs(n_samples=1000, centers=((4, -4), (0, 0)), random_state=42)\n",
        "X1 = X1.dot(np.array([[0.374, 0.95], [0.732, 0.598]]))\n",
        "X2, y2 = make_blobs(n_samples=250, centers=1, random_state=42)\n",
        "X2 = X2 + [6, -8]\n",
        "X = np.r_[X1, X2]\n",
        "y = np.r_[y1, y2]\n",
        "\n",
        "plot_clusters(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kIqAvwAafzk2"
      },
      "source": [
        "# modelo 1 (esquerda)\n",
        "dbscan1 = DBSCAN(eps=0.25, min_samples=5)\n",
        "dbscan1.fit(X)\n",
        "\n",
        "# modelo 2 (direita)\n",
        "dbscan2 = DBSCAN(eps=0.35, min_samples=5)\n",
        "dbscan2.fit(X)\n",
        "\n",
        "plt.figure(figsize=(12, 4))\n",
        "plt.subplot(121)\n",
        "plot_dbscan(dbscan1, X, size=200)\n",
        "plt.subplot(122)\n",
        "plot_dbscan(dbscan2, X, size=600)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}